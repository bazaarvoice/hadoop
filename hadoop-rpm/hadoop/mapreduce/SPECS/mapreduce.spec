# rpmrebuild autogenerated specfile

%define defaultbuildroot /
AutoProv: no
%undefine __find_provides
AutoReq: no
%undefine __find_requires
# Do not try autogenerate prereq/conflicts/obsoletes and check files
%undefine __check_files
%undefine __find_prereq
%undefine __find_conflicts
%undefine __find_obsoletes
# Be sure buildpolicy set to do nothing
%define __spec_install_post %{nil}
# Something that need for rpm-4.1
%define _missing_doc_files_terminate_build 0

# HDP specific parameters
%define hdp_version 2.6.1.0
%define hadoop_src /root/hadoop
%define hadoop_version 3.1.1-SNAPSHOT

BuildArch:     x86_64
Name:          hadoop_2_6_1_0_129-mapreduce
Version:       2.7.3.2.6.1.0
Release:       129
License:       Apache License v2.0 
Group:         System/Daemons
Summary:       The Hadoop MapReduce (MRv2)


URL:           http://hadoop.apache.org/core/







Provides:      hadoop_2_6_1_0_129-mapreduce = 2.7.3.2.6.1.0-129
Provides:      hadoop_2_6_1_0_129-mapreduce(x86-64) = 2.7.3.2.6.1.0-129
Requires:      hadoop_2_6_1_0_129-yarn = 2.7.3.2.6.1.0-129
Requires:      /bin/sh  
#Requires:      rpmlib(FileDigests) <= 4.6.0-1
#Requires:      rpmlib(PayloadFilesHavePrefix) <= 4.0-1
#Requires:      rpmlib(CompressedFileNames) <= 3.0.4-1
Requires:      /bin/bash  
Requires:      /usr/bin/env  
#Requires:      rpmlib(PayloadIsXz) <= 5.2-1
#suggest
#enhance
%description
Hadoop MapReduce is a programming model and software framework for writing applications
that rapidly process vast amounts of data in parallel on large clusters of compute nodes.
%prep -p /bin/sh
# make sure that the current build dir is configured under /usr/hdp

echo "Build root: $RPM_BUILD_ROOT"
echo "HDP version: %{hdp_version}"
echo "Release: %{release}"
echo "Hadoop src: %{hadoop_src}"
echo "Hadoop version: %{hadoop_version}"

%define hdp_build %hdp_version-%release
%define hdp /usr/hdp/%hdp_version-%release
%define jar_version %{hadoop_version}.%{hdp_build}
HADOOP_DIST=%hadoop_src/hadoop-dist/target/hadoop-%hadoop_version

if [ ! -d "$RPM_BUILD_ROOT" ]; then
  echo "Build root $RPM_BUILD_ROOT doesn't exist"
  mkdir -p $RPM_BUILD_ROOT
fi

if [ ! -d "$HADOOP_DIST" ]; then
  echo "Hadoop distribution source doesn't exist. Make sure that a version of Hadoop is built from source and is available under hadoop-dist module at $HADOOP_DIST"
  exit -1;
fi

# cleanup everything before we start
rm -rf $RPM_BUILD_ROOT/*

# create a current HDP dir
mkdir -p $RPM_BUILD_ROOT%{hdp}

# copy required hadoop only files from HADOOP_SRC to BUILDROOT
HDP_OUT=$RPM_BUILD_ROOT%hdp
OUT=$RPM_BUILD_ROOT%hdp/hadoop-mapreduce
DIST=$HADOOP_DIST
SRC=%hadoop_src

# hadoop conf.empty for mapreduce
mkdir -p $HDP_OUT/etc/hadoop/conf.empty
cp $DIST/etc/hadoop/mapred-env.sh $HDP_OUT/etc/hadoop/conf.empty
cp $DIST/etc/hadoop/mapred-queues.xml.template $HDP_OUT/etc/hadoop/conf.empty
cp $DIST/etc/hadoop/mapred-site.xml $HDP_OUT/etc/hadoop/conf.empty

# mapreduce security limits
mkdir -p $HDP_OUT/etc/security/limits.d
cat > $HDP_OUT/etc/security/limits.d/mapreduce.conf <<"EOL"
mapred    - nofile 32768
mapred    - nproc  65536
EOL

# hadoop-mapreduce/bin
mkdir -p $OUT/bin
cp $DIST/bin/mapred $OUT/bin/mapred.distro
sed -i 's/HADOOP_SHELL_EXECNAME="\${MYNAME\#\#\*\/}"/HADOOP_SHELL_EXECNAME="mapred"/g' $OUT/bin/mapred.distro

# setup HDP specific mapreduce shell
cat > $OUT/bin/mapred <<"EOL"
#!/bin/bash

export HADOOP_HOME=${HADOOP_HOME:-%{hdp}/hadoop}
export HADOOP_MAPRED_HOME=${HADOOP_MAPRED_HOME:-%{hdp}/hadoop-mapreduce}
export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-%{hdp}/hadoop-yarn}
export HADOOP_LIBEXEC_DIR=${HADOOP_HOME}/libexec
export HDP_VERSION=${HDP_VERSION:-%{hdp_build}}
export HADOOP_OPTS="${HADOOP_OPTS} -Dhdp.version=${HDP_VERSION}"
export YARN_OPTS="${YARN_OPTS} -Dhdp.version=${HDP_VERSION}"

exec %{hdp}/hadoop-mapreduce/bin/mapred.distro "$@"
EOL

# setup HDP specific mapreduce shell
mkdir -p $HDP_OUT/hadoop/bin
cat > $HDP_OUT/hadoop/bin/mapred <<"EOL"
#!/bin/bash

export HADOOP_HOME=${HADOOP_HOME:-%{hdp}/hadoop}
export HADOOP_MAPRED_HOME=${HADOOP_MAPRED_HOME:-%{hdp}/hadoop-mapreduce}
export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-%{hdp}/hadoop-yarn}
export HADOOP_LIBEXEC_DIR=${HADOOP_HOME}/libexec
export HDP_VERSION=${HDP_VERSION:-%{hdp_build}}
export HADOOP_OPTS="${HADOOP_OPTS} -Dhdp.version=${HDP_VERSION}"
export YARN_OPTS="${YARN_OPTS} -Dhdp.version=${HDP_VERSION}"

exec %{hdp}/hadoop-mapreduce/bin/mapred.distro "$@"
EOL

# hadoop/libexec/mapred-config.sh
mkdir -p $HDP_OUT/hadoop/libexec
cp $DIST/libexec/mapred-config.sh $HDP_OUT/hadoop/libexec

# hadoop-mapreduce/sbin
mkdir -p $OUT/sbin
cp $DIST/sbin/mr-jobhistory-daemon.sh $OUT/sbin
sed -i 's/#!\/usr\/bin\/env bash/#!\/usr\/bin\/env bash\n\nexport HADOOP_HOME=\/usr\/hdp\/%{hdp_build}\/hadoop/g' $OUT/sbin/mr-jobhistory-daemon.sh

# hadoop-mapreduce common jars
declare -A cj=(
  [hadoop-mapreduce-client-app]=share/hadoop/mapreduce/hadoop-mapreduce-client-app-%{hadoop_version}.jar
  [hadoop-mapreduce-client-common]=share/hadoop/mapreduce/hadoop-mapreduce-client-common-%{hadoop_version}.jar
  [hadoop-mapreduce-client-core]=share/hadoop/mapreduce/hadoop-mapreduce-client-core-%{hadoop_version}.jar
  [hadoop-mapreduce-client-jobclient]=share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-%{hadoop_version}.jar
  [hadoop-mapreduce-client-jobclient-tests]=share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-%{hadoop_version}-tests.jar
  [hadoop-mapreduce-client-nativetask]=share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-%{hadoop_version}.jar
  [hadoop-mapreduce-client-hs]=share/hadoop/mapreduce/hadoop-mapreduce-client-hs-%{hadoop_version}.jar
  [hadoop-mapreduce-client-hs-plugins]=share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-%{hadoop_version}.jar
  [hadoop-mapreduce-client-shuffle]=share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-%{hadoop_version}.jar
  [hadoop-mapreduce-client-uploader]=share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-%{hadoop_version}.jar
  [hadoop-mapreduce-examples]=share/hadoop/mapreduce/hadoop-mapreduce-examples-%{hadoop_version}.jar
)
pushd $OUT
# hadoop-mapreduce
for f in "${!cj[@]}"; do
  p=${cj[${f}]}
  cp $DIST/$p $f-%{jar_version}.jar
  ln -s $f-%{jar_version}.jar $f.jar
done
# hadoop-tools/lib -> hadoop-mapreduce
cp -f $DIST/share/hadoop/tools/lib/*.jar ./

# hadoop-tools/yarn -> hadoop-mapreduce
declare -A yj=(
  [hadoop-yarn-api]=share/hadoop/yarn/hadoop-yarn-api-%{hadoop_version}.jar
  [hadoop-yarn-common]=share/hadoop/yarn/hadoop-yarn-common-%{hadoop_version}.jar
  [hadoop-yarn-registry]=share/hadoop/yarn/hadoop-yarn-registry-%{hadoop_version}.jar
  [hadoop-yarn-server-common]=share/hadoop/yarn/hadoop-yarn-server-common-%{hadoop_version}.jar
  [hadoop-yarn-server-resourcemanager]=share/hadoop/yarn/hadoop-yarn-server-resourcemanager-%{hadoop_version}.jar
  [hadoop-yarn-server-web-proxy]=share/hadoop/yarn/hadoop-yarn-server-web-proxy-%{hadoop_version}.jar
)
# hadoop-yarn
for f in "${!yj[@]}"; do
  p=${yj[${f}]}
  cp $DIST/$p $f-%{jar_version}.jar
  ln -s $f-%{jar_version}.jar $f.jar
done
cp -f $DIST/share/hadoop/yarn/lib/*.jar ./

# hadoop common lib -> hadoop-mapreduce
cp -f $DIST/share/hadoop/common/lib/*.jar ./
popd

%files
# default attribues
%defattr(0644, root, root, 0755)

# hdfs config
%config(noreplace) %attr(0644, root, root) "%hdp/etc/hadoop/conf.empty"
%config(noreplace) %attr(0644, root, root) "%hdp/etc/security/limits.d"

# everything else
%attr(0755, root, root) "%hdp/hadoop-mapreduce/*"
%attr(0755, root, root) "%hdp/hadoop/*"

%pre -p /bin/sh
getent group mapred >/dev/null || groupadd -r mapred
getent passwd mapred >/dev/null || /usr/sbin/useradd --comment "Hadoop MapReduce" --shell /bin/bash -M -r -g mapred -G hadoop --home /var/lib/hadoop-mapreduce mapred

if [[ ! -e "/var/log/hadoop-mapreduce" ]]; then
    /usr/bin/install -d -o mapred -g hadoop -m 0775  /var/log/hadoop-mapreduce
fi

if [[ ! -e "/var/run/hadoop-mapreduce" ]]; then
    /usr/bin/install -d -o mapred -g hadoop -m 0775  /var/run/hadoop-mapreduce
fi

if [[ ! -e "/var/lib/hadoop-mapreduce" ]]; then
    /usr/bin/install -d -o mapred -g hadoop -m 0755  /var/lib/hadoop-mapreduce
fi

if [[ ! -e "/var/lib/hadoop-mapreduce/cache" ]]; then
    /usr/bin/install -d -o mapred -g hadoop -m 1777  /var/lib/hadoop-mapreduce/cache
fi
%changelog
